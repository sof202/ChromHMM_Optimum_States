"use strict";(self.webpackChunkdocumentation=self.webpackChunkdocumentation||[]).push([[2790],{8772:(e,i,t)=>{t.r(i),t.d(i,{assets:()=>l,contentTitle:()=>s,default:()=>c,frontMatter:()=>r,metadata:()=>a,toc:()=>h});var n=t(5893),o=t(1151);const r={sidebar_position:7},s="Memory profiling",a={id:"ChromOptimise/Memory-Profiling",title:"Memory profiling",description:"To gain insights into the peak heap consumption of each script in the pipeline,",source:"@site/docs/ChromOptimise/Memory-Profiling.md",sourceDirName:"ChromOptimise",slug:"/ChromOptimise/Memory-Profiling",permalink:"/ChromOptimise/ChromOptimise/Memory-Profiling",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:7,frontMatter:{sidebar_position:7},sidebar:"documentationSidebar",previous:{title:"Processing times",permalink:"/ChromOptimise/ChromOptimise/Processing-Times"},next:{title:"SLURM information",permalink:"/ChromOptimise/ChromOptimise/SLURM-Workload-Manager-Information"}},l={},h=[{value:"1_BinarizeBamFiles.sh",id:"1_binarizebamfilessh",level:2},{value:"2_batch_CreateIncrementalModels.sh",id:"2_batch_createincrementalmodelssh",level:2},{value:"3_OptimalNumberofStates.sh",id:"3_optimalnumberofstatessh",level:2},{value:"4_ReferenceLDSCore.sh",id:"4_referenceldscoresh",level:2},{value:"5_PartitionedHeritability.sh",id:"5_partitionedheritabilitysh",level:2},{value:"Generate_Big_Model.sh",id:"generate_big_modelsh",level:2}];function m(e){const i={a:"a",code:"code",h1:"h1",h2:"h2",li:"li",p:"p",ul:"ul",...(0,o.a)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(i.h1,{id:"memory-profiling",children:"Memory profiling"}),"\n",(0,n.jsx)(i.p,{children:"To gain insights into the peak heap consumption of each script in the pipeline,\nwe utilized the tool Heaptrack. This allowed for the estimation of memory\nconsumption based on unit tests (albeit only with relatively small amounts of\ndata). Utilizing this information will empower the user to make well-informed\ndecisions regarding the allocation of memory for the scripts involved in the\npipeline and ChromHMM."}),"\n",(0,n.jsxs)(i.p,{children:["Heaptrack, produced by KDE, can be obtained through an appimage download or\nlocal compilation. The GitHub repository for Heaptrack can be accessed\n",(0,n.jsx)(i.a,{href:"https://github.com/KDE/heaptrack",children:"here"}),"."]}),"\n",(0,n.jsx)(i.h2,{id:"1_binarizebamfilessh",children:"1_BinarizeBamFiles.sh"}),"\n",(0,n.jsxs)(i.p,{children:["ChromHMM's ",(0,n.jsx)(i.code,{children:"BinarizeBam"})," command is the main contributor to memory consumption\nin this script."]}),"\n",(0,n.jsx)(i.p,{children:"In testing, the following was observed:"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"For a directory with 16.8 GB of .bam files: peak heap memory consumption was\n40 MB"}),"\n"]}),"\n",(0,n.jsx)(i.h2,{id:"2_batch_createincrementalmodelssh",children:"2_batch_CreateIncrementalModels.sh"}),"\n",(0,n.jsxs)(i.p,{children:["ChromHMM's ",(0,n.jsx)(i.code,{children:"LearnModel"})," command is the main contributor to memory consumption\nin this script."]}),"\n",(0,n.jsx)(i.p,{children:"In testing, the following was observed:"}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:["Using binary files that were 1 MB in total:","\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"2 state model training had 181 MB peak memory consumption"}),"\n",(0,n.jsx)(i.li,{children:"3 state model training had 145 MB peak memory consumption"}),"\n",(0,n.jsx)(i.li,{children:"4 state model training had 165 MB peak memory consumption"}),"\n",(0,n.jsx)(i.li,{children:"5 state model training had 128 MB peak memory consumption"}),"\n",(0,n.jsx)(i.li,{children:"6 state model training had 165 MB peak memory consumption"}),"\n",(0,n.jsx)(i.li,{children:"7 state model training had 168 MB peak memory consumption"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(i.p,{children:"Based on the provided information, it is evident that the memory consumption is\nprimarily dependent on the total size of the binary files rather than the\nnumber of states used in the models."}),"\n",(0,n.jsx)(i.p,{children:"Additionally, it's important to note that this script is designed to be\nexecuted as an array through the SLURM workload manager. Therefore, the peak\nmemory consumption is anticipated to be proportional to the size of the largest\n.bam files multiplied by the number of array elements."}),"\n",(0,n.jsx)(i.h2,{id:"3_optimalnumberofstatessh",children:"3_OptimalNumberofStates.sh"}),"\n",(0,n.jsx)(i.p,{children:"This script utilizes Rscripts which entail minor computations, resulting in\nvery low memory consumption. Through testing, it has been observed that the\nmemory usage never surpassed a few kilobytes."}),"\n",(0,n.jsx)(i.h2,{id:"4_referenceldscoresh",children:"4_ReferenceLDSCore.sh"}),"\n",(0,n.jsx)(i.p,{children:"This script's biggest requirement of memory comes from the SNP assigment R\nscript that it calls. In testing, chromosomes 1 and 2 use about 800MB of\nmemory. Because the script is ran as an array that processes each chromosome at\nthe same time, 22 GB of memory is allocated."}),"\n",(0,n.jsx)(i.h2,{id:"5_partitionedheritabilitysh",children:"5_PartitionedHeritability.sh"}),"\n",(0,n.jsxs)(i.p,{children:["According to ",(0,n.jsx)(i.a,{href:"https://github.com/bulik/ldsc/wiki/FAQ",children:"ldsc's creators"}),",\npartitioned heritability should only take up ~8GB for 50 annotations. The number\nof annotations in this script will be greater than 50. In testing, over 20GB is\nrequired to stop out of memory errors for just 58 annotations. To account for\nthis, the SLURM directive is set to 50GB of memory."]}),"\n",(0,n.jsx)(i.h2,{id:"generate_big_modelsh",children:"Generate_Big_Model.sh"}),"\n",(0,n.jsxs)(i.p,{children:["This script is similar to\n",(0,n.jsx)(i.a,{href:"#2_batch_createincrementalmodelssh",children:"2_batch_CreateIncrementalModels.sh"}),",\nsuggesting that the memory consumption pattern is likely to align with the\nconclusions drawn there. However, it's important to note that due to the\ncomputational time required for this script, utilizing heaptrack to monitor\nmemory allocations was impractical. Consequently, the memory consumption for\nnotably large models (20 states or more) remains unknown and is not extensively\ndocumented."]})]})}function c(e={}){const{wrapper:i}={...(0,o.a)(),...e.components};return i?(0,n.jsx)(i,{...e,children:(0,n.jsx)(m,{...e})}):m(e)}},1151:(e,i,t)=>{t.d(i,{Z:()=>a,a:()=>s});var n=t(7294);const o={},r=n.createContext(o);function s(e){const i=n.useContext(r);return n.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function a(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),n.createElement(r.Provider,{value:i},e.children)}}}]);